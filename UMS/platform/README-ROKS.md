# Install User Management Service 19.0.2 on Red Hat OpenShift on IBM Cloud

User Management Service can be installed on Red Hat OpenShift cluster on IBM Cloud.
This documentation provides a step-by-step instruction on how to install UMS on Red Hat OpenShift cluster on IBM Cloud for test purposes. The documentation therefore does not include steps to setup a production-ready database, create image policy or configure persistent volume.

## Prepare your environment

Refer to [Red Hat OpenShift on IBM Cloud](https://cloud.ibm.com/docs/openshift?topic=openshift-openshift-create-cluster#openshift_create_cluster_console) documentation to install IBM Cloud and OpenShift CLIs and to create an OpenShift cluster in IBM Cloud.

Log in to IBM Cloud by running the command
```
ibmcloud login --sso
```

Login to IBM Cloud Container Registry by running the command
```
ibmcloud cr login
```

## Prerequisites

### Create a database
This is optional. As this is the instruction for a test deployment of UMS, UMS will use the built-in derby database.

### Create namespace and switch to use it

1. In a browser, navigate to https://cloud.ibm.com/kubernetes/clusters. Login with your IBM Cloud ID.
2. For your Red Hat OpenShift cluster select `...` and click `OpenShift Web Console`.
3. In the OpenShift Web Console click on your user ID (top right) and click Copy Login Command.
4. Paste the login command into a shell
```
oc login <hostname> --token=<token>
```
5. Create and switch to the namespace you created by using the command
```
oc new-project cp4a-ums
```
You see the message "Now using project cp4a-ums on server <hostname>".

### Create image policy
This is optional. As this is the instruction for a test deployment of UMS, creating image policies is not covered.

### Create a docker pull secret
1. In the IBM Cloud Console, select Manage / Access (IAM) (upper right corner)
2. In the menu on the left site click on `Service IDs`, then click `Create` - enter a name e.g. `ums-serviceid` and description.
3. Select API keys (right tab) and click Create - enter name ums-apikey and description ums-eval-api-key
4. Download the API key as a json file.
5. Create a docker pull secret in your OpenShift cluster:
```
oc create secret docker-registry ums-secret --docker-server=us.icr.io --docker-username=iamapikey --docker-password=<apiKey from downloaded apiKey.json>
```

**Note** this secret will be passed to the chart via the `imagePullSecrets` property.

### Generate TLS secret
To ensure the internal communication is secure, a TLS secret must be provided.
The secret can be generated by running the following command:
```bash
openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt
```

This command generates two files: tls.crt and tls.key. They are used to generate the TLS secret:
```bash
kubectl create secret tls ibm-dba-ums-tls --key=tls.key --cert=tls.crt
```

**Note**: The secret will be passed to the chart via the `tls.tlsSecretName` property.


### Generate UMS secret, DB secrets and LTPA generation secret
To avoid passing sensitive information via `myvalues.yaml`, three secrets need to be generated before installing the chart.
1. Edit [ums-secret.yaml](../configuration/ums-secret.yaml)
2. For ibm-dba-ums-secret specify adminUser, adminPassword, sslKeystorePassword, jwtKeystorePassword, teamserverClientID, teamserverClientSecret and ltpaPassword
3. For ibm-dba-ums-db-secret specify oauthDBUser/outhDBPassword and tsDBUser/tsDBPassword.
4. For ibm-dba-ums-ltpa-creation-secret do nothing. Configuration will be performed during LTPA creation.
5. Save ums-secret.yaml
6. In a shell run this command to create the required secrets.
```
kubectl create -f ums-secret.yaml --namespace cp4a-ums
```

**Note**: Secret names need to be passed to the chart via the global.ums.adminSecretName, global.ums.dbSecretName and global.ums.ltpaSecretName properties.

### Install IBM Cloud Pak SecurityContextConstraints resources to your cluster
Install IBM Cloud Pak SecurityContextConstraints resources to your cluster. Refer to '[`ibm-restricted-scc`](https://ibm.biz/cpkspec-scc)'.

### Persistent Volume
This is optional. As this is the instruction for a test deployment of UMS, Persistent Volume configuration is not covered.

## Install the chart

### Download PPA and load images to the content registry
1. Follow instructions to download User Management Service images and loadimages.sh file in [Download PPA and load images](https://github.com/icp4a/cert-kubernetes/blob/master/README.md#step-2-download-a-product-package-from-ppa-and-load-the-images)
2. Load images to the IBM Cloud container repository
```
loadimages.sh -p K8S_UMS*.tgz -r us.icr.io/cp4a-ums
```

When finished, you see a message similar to:
```
Docker images push to us.icr.io/cp4a-ums completed, and check the following images in the Docker registry:
     -  us.icr.io/cp4a-ums/ums:19.0.2
     -  us.icr.io/cp4a-ums/dba-keytool-initcontainer:19.0.2
     -  us.icr.io/cp4a-ums/dba-keytool-jobcontainer:19.0.2
```
Those image names must match the images section in `myvalues.yaml`.

### Download helm chart and customize values.yaml
1. Download the helm chart [ibm-dba-ums-prod-1.0.0.tgz](../helm-charts/ibm-dba-ums-prod-1.0.0.tgz)
2. In a shell extract the downloaded package
```bash
tar -xvf ibm-dba-ums-prod-1.0.0.tgz
```
3. Review `values.yaml` and override defaults where necessary to meet your environment and configuration. 
Review README.md inside the helm chart for more details on the individual settings. 
Make sure to set the global.isOpenShift parameter to true. This ensures required configuration for the pod's container security context.
Save the new configuration as `myvalues.yaml`.

*Note:* Minimal changes to `myvalues.yaml` include specifying serviceType, imagePullSecrets, adminSecretName, dbSecretName, ltpaSecretName, images location, tlsSecretName, 
database type (if using derby, name, host and port are ignored). Hostname is not needed, it will be configured when the route is defined in the OpenShift environment. 
See sample below:

```yaml
# shared values across components
global:
  # PersistenceVolumeClaim name with JDBC drivers
  existingClaimName:
  # Secret with Docker credentials
  imagePullSecrets: ums-secret
  # Set to false if you are not using Openshift
  isOpenShift: true
  # UMS-specific global values
  ums:
    serviceType: Ingress
    #  hostname: c1-e.us-east.containers.cloud.ibm.com
    port: 443
    # Secret with admin credentials
    adminSecretName: ibm-dba-ums-secret
    # Secret with DB connection credentials
    dbSecretName: ibm-dba-ums-db-secret
    #Secret to be filled from the LTPA creation job
    ltpaSecretName: ibm-dba-ums-ltpa-creation-secret

# UMS Docker images
images:
  ums: us.icr.io/cp4a-ums/ums:19.0.2
  initTLS: us.icr.io/cp4a-ums/dba-keytool-initcontainer:19.0.2
  ltpa: us.icr.io/cp4a-ums/dba-keytool-jobcontainer:19.0.2

# Secret with an Ingress certificate
ingressSecretName:

# UMS certificate secret
tls:
  tlsSecretName: ibm-dba-ums-tls

# Toggle for custom JDBC drivers
useCustomJDBCDrivers: false

# UMS OAuth config
oauth:
  database:
    type: derby
    # name:
    # host:
    # port:
    driverfiles:
  clientManagerGroup:
  jwtSecretName: ibm-dba-ums-tls

# UMS Team Server database config
teamserver:
  database:
    type: derby
    # name:
    # host:
    # port:
    driverfiles:
```

### Generate and customize deployment yamls
1. Generate the output folder
```
mkdir yamls
```
2. Generate deployment yamls to the created folder
```
helm template --name cp4a-ums --namespace cp4a-ums --output-dir ./yamls -f myvalues.yaml ibm-dba-ums-prod-1.0.0.tgz
```
3. Move to the yamls folder. Remove `ibm-dba-ums-prod/templates/test` folder.
4. Apply yaml definitions by running the command
```
kubectl apply -R -f ./yamls
```
Your output should look similar to:
```
role.rbac.authorization.k8s.io/cp4a-ums-ibm-dba-ums-deployment created
rolebinding.rbac.authorization.k8s.io/cp4a-ums-ibm-dba-ums-deployment created
serviceaccount/cp4a-ums-ibm-dba-ums created
role.rbac.authorization.k8s.io/cp4a-ums-ibm-dba-ums-ltpa-creation-role created
rolebinding.rbac.authorization.k8s.io/cp4a-ums-ibm-dba-ums-ltpa-creation-role-binding created
serviceaccount/cp4a-ums-ibm-dba-ums-ltpa-creation-service-account created
networkpolicy.networking.k8s.io/ums-apiserver created
networkpolicy.networking.k8s.io/ums-database created
networkpolicy.networking.k8s.io/default-deny created
networkpolicy.networking.k8s.io/ums-dns created
networkpolicy.networking.k8s.io/ums-https created
networkpolicy.networking.k8s.io/ums-ldap created
networkpolicy.networking.k8s.io/ums-test-container-https created
configmap/cp4a-ums-ibm-dba-ums created
configmap/cp4a-ums-ibm-dba-ums-custom created
deployment.apps/cp4a-ums-ibm-dba-ums created
horizontalpodautoscaler.autoscaling/cp4a-ums-ibm-dba-ums created
job.batch/cp4a-ums-ibm-dba-ums-ltpa-creation-job-39987 created
poddisruptionbudget.policy/cp4a-ums-ibm-dba-ums created
service/cp4a-ums-ibm-dba-ums created
```
### Create a route to expose User Management Service
1. In a browser login to IBM Cloud, select your cluster and open the OpenShift web console. Select your application (cp4a-ums in this example).
2. From the menu select Applications -> Routes. Click `Create Route`.
3. Provide a uniqu name for the route, e.g. `cp4a-ums-route`.
4. Leave the Hostname black, it will be generated.
6. As Path specify `/ums`
7. Select the service and the Target Port (9444 -> 9443 (TCP))
8. Check the box `Secure route`
9. For TLS Termination select `Re-encrypt`
10. For Insecure Traffic specify `None`
11. As CA Certificate, provide the certificate you used to generate the TLS secret
12. Click `Create` to create the route.

### Configure hostname in the Config Map
1. Copy the hostname that was generated for the route in the previous step.
2. In the OpenShift console of your application, select Ressources -> Config Maps.
3. Select the Config Map.
4. Click on Actions -> Edit YAML.
5. In section `ums.xml` for the variable name `ums.externalHostName` specify the value of the generated hostname.
6. Save the Config Map.

## Verify UMS installation
From the Routes view click on the Hostname that was generated for the route.
UMS Login page opens in the browser. Log in as the administrative user you specified in ums-secret.yaml
or any user of a connected LDAP if you included an LDAP configuration in myvalues.yaml customXML.

Congratulations, your UMS is now on ROKS.
