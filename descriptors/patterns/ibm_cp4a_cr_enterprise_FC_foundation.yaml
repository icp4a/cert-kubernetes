###############################################################################
##
##Licensed Materials - Property of IBM
##
##(C) Copyright IBM Corp. 2021. All Rights Reserved.
##
##US Government Users Restricted Rights - Use, duplication or
##disclosure restricted by GSA ADP Schedule Contract with IBM Corp.
##
###############################################################################
apiVersion: icp4a.ibm.com/v1
kind: ICP4ACluster
metadata:
  name: icp4adeploy
  labels:
    app.kubernetes.io/instance: ibm-dba
    app.kubernetes.io/managed-by: ibm-dba
    app.kubernetes.io/name: ibm-dba
    release: 21.0.1
spec:

  ## MUST exist, used to accept ibm license, valid value only can be "accept"
  ibm_license: ""

  appVersion: 21.0.1
  ##########################################################################
  ## This section contains the shared configuration for all CP4A components #
  ##########################################################################
  shared_configuration:

    ## FileNet Content Manager (FNCM) license and possible values are: user, non-production, and production.
    ## This value could be different from the other licenses in the CR.
    sc_deployment_fncm_license: "<Required>"

    ## Business Automation Workflow (BAW) license and possible values are: user, non-production, and production.
    ## This value could be different from the other licenses in the CR.
    sc_deployment_baw_license: "<Required>"

    ## Use this parameter to specify the license for the CP4A deployment and
    ## the possible values are: non-production and production and if not set, the license will
    ## be defaulted to production.  This value could be different from the other licenses in the CR.
    sc_deployment_license: "<Required>"

    ## All CP4A components must use/share the image_pull_secrets to pull images
    image_pull_secrets:
    - admin.registrykey

    ## All CP4A components must use/share the same docker image repository.  For example, if IBM Entitled Registry is used, then
    ## it should be "cp.icr.io".  Otherwise, it will be a local docker registry.
    sc_image_repository: cp.icr.io

    ## For non-OCP (e.g., CNCF platforms such as AWS, GKE, etc), this parameter is required
    sc_run_as_user:

    images:
      keytool_job_container:
        repository: cp.icr.io/cp/cp4a/ums/dba-keytool-jobcontainer
        tag: 21.0.1
      dbcompatibility_init_container:
        repository: cp.icr.io/cp/cp4a/aae/dba-dbcompatibility-initcontainer
        tag: 21.0.1
      keytool_init_container:
        repository: cp.icr.io/cp/cp4a/ums/dba-keytool-initcontainer
        tag: 21.0.1
      umsregistration_initjob:
        repository: cp.icr.io/cp/cp4a/aae/dba-umsregistration-initjob
        tag: 21.0.1

      ## All CP4A components should use this pull_policy as the default, but it can override by each component
      pull_policy: IfNotPresent

    ## All CP4A components must use/share the root_ca_secret in order for integration
    root_ca_secret: icp4a-root-ca

    ## CP4A patterns or capabilities to be deployed.  This CR represents the "foundation" pattern, which includes the following
    ## mandatory components: icn (BAN/Navigator), rr (Resource Registry) and optional components: ums, bas, and bai
    sc_deployment_patterns: foundation

    ## The optional components to be installed if listed here.  This is normally populated by the User script based on input from the user.  User can
    ## also manually specify the optional components to be deployed here.  For this foundation CR, the optional components are: ums, bas and bai
    sc_optional_components:

    ## The deployment type as selected by the user.  Possible values are: demo, enteprise
    sc_deployment_type: enterprise

    ## The platform to be deployed specified by the user.  Possible values are: OCP and other.  This is normally populated by the User script
    ## based on input from the user.
    sc_deployment_platform:

    ## For OCP, this is used to create route, you should input a valid hostname in the required field.
    sc_deployment_hostname_suffix: "{{ meta.namespace }}.<Required>"

    ## For ROKS, this is used to enable the creation of ingresses. The default value is "false", which routes will be created.
    sc_ingress_enable: false

    ## For ROKS Ingress, provide TLS secret name for Ingress controller.
    sc_ingress_tls_secret_name: <Required>

    ## If the root certificate authority (CA) key of the external service is not signed by the operator root CA key, provide the TLS certificate of
    ## the external service to the component's truststore.
    trusted_certificate_list: []

    ## Shared encryption key secret name that is used for Workflow or Workstream Services and Process Federation Server integration.
    ## This secret is also used by Workflow and BAStudio to store AES encryption key.
    encryption_key_secret: ibm-iaws-shared-key-secret

    ## On OCP 3.x and 4.x, the User script will populate these three (3) parameters based on your input for "enterprise" deployment.
    ## If you manually deploying without using the User script, then you would provide the different storage classes for the slow, medium
    ## and fast storage parameters below.  If you only have 1 storage class defined, then you can use that 1 storage class for all 3 parameters.
    storage_configuration:
      sc_slow_file_storage_classname: "<Required>"
      sc_medium_file_storage_classname: "<Required>"
      sc_fast_file_storage_classname: "<Required>"

  ## The beginning section of LDAP configuration for CP4A
  ldap_configuration:
    ## The possible values are: "IBM Security Directory Server" or "Microsoft Active Directory"
    lc_selected_ldap_type: "<Required>"

    ## The name of the LDAP server to connect
    lc_ldap_server: "<Required>"

    ## The port of the LDAP server to connect.  Some possible values are: 389, 636, etc.
    lc_ldap_port: "<Required>"

    ## The LDAP bind secret for LDAP authentication.  The secret is expected to have ldapUsername and ldapPassword keys.  Refer to Knowledge Center for more info.
    lc_bind_secret: ldap-bind-secret

    ## The LDAP base DN.  For example, "dc=example,dc=com", "dc=abc,dc=com", etc
    lc_ldap_base_dn: "<Required>"

    ## Enable SSL/TLS for LDAP communication. Refer to Knowledge Center for more info.
    lc_ldap_ssl_enabled: true

    ## The name of the secret that contains the LDAP SSL/TLS certificate.
    lc_ldap_ssl_secret_name: "<Required>"

    ## The LDAP user name attribute.  One possible value is "*:cn" for TDS and "user:sAMAccountName" for AD. Refer to Knowledge Center for more info.
    lc_ldap_user_name_attribute: "<Required>"

    ## The LDAP user display name attribute. One possible value is "cn" for TDS and "sAMAccountName" for AD. Refer to Knowledge Center for more info.
    lc_ldap_user_display_name_attr: "<Required>"

    ## The LDAP group base DN.  For example, "dc=example,dc=com", "dc=abc,dc=com", etc
    lc_ldap_group_base_dn: "<Required>"

    ## The LDAP group name attribute.  One possible value is "*:cn" for TDS and "*:cn" for AD. Refer to Knowledge Center for more info.
    lc_ldap_group_name_attribute: "*:cn"

    ## The LDAP group display name attribute.  One possible value for both TDS and AD is "cn". Refer to Knowledge Center for more info.
    lc_ldap_group_display_name_attr: "cn"

    ## The LDAP group membership search filter string.  One possible value is "(&(cn=%v)(|(objectclass=groupOfNames)(objectclass=groupOfUniqueNames)(objectclass=groupOfURLs)))" for TDS
    ## and "(&(cn=%v)(objectcategory=group))" for AD.
    lc_ldap_group_membership_search_filter: "<Required>"

    ## The LDAP group membership ID map.  One possible value is "groupofnames:member" for TDS and "memberOf:member" for AD.
    lc_ldap_group_member_id_map: "<Required>"
    
    ## The LDAP recursive search. Set true or false to enable or disable recursive searches.
    lc_ldap_recursive_search: false
    
    ## The maximum search results. Specify a higher value if you expect more search results.
    lc_ldap_max_search_results: 4500

    ## The User script will uncomment the section needed based on user's input from User script.  If you are deploying without the User script,
    ## uncomment the necessary section (depending if you are using Active Directory (ad) or Tivoli Directory Service (tds)) accordingly.
    # ad:
    #   lc_ad_gc_host: "<Required>"
    #   lc_ad_gc_port: "<Required>"
    #   lc_user_filter: "(&(sAMAccountName=%v)(objectcategory=user))"
    #   lc_group_filter: "(&(cn=%v)(objectcategory=group))"
    # tds:
    #   lc_user_filter: "(&(cn=%v)(objectclass=person))"
    #   lc_group_filter: "(&(cn=%v)(|(objectclass=groupofnames)(objectclass=groupofuniquenames)(objectclass=groupofurls)))"

  ## The beginning section of database configuration for CP4A
  datasource_configuration:
    ## The dc_ssl_enabled parameter is used to support database connection over SSL for DB2/Oracle.
    dc_ssl_enabled: true
    ## The database_precheck parameter is used to enable or disable CPE/Navigator database connection check.
    ## If set to "true", then CPE/Navigator database connection check will be enabled.
    ## if set to "false", then CPE/Navigator database connection check will not be enabled.
   # database_precheck: true
    ## The database configuration for ICN (Navigator) - aka BAN (Business Automation Navigator)
    dc_icn_datasource:
      ## Provide the database type from your infrastructure.  The possible values are "db2" or "db2HADR" or "oracle".  This should be the same as the
      ## GCD and object store configuration above.
      dc_database_type: "<Required>"
      ## Provide the ICN datasource name.  The default value is "ECMClientDS".
      dc_common_icn_datasource_name: "ECMClientDS"
      database_servername: "<Required>"
      ## Provide the database server port.  For Db2, the default is "50000".  For Oracle, the default is "1521"
      database_port: "<Required>"
      ## Provide the name of the database for ICN (Navigator).  For example: "ICNDB"
      database_name: "<Required>"
      ## The name of the secret that contains the DB2 SSL certificate.
      database_ssl_secret_name: "<Required>"
      ## If the database type is Oracle, provide the Oracle DB connection string.  For example, "jdbc:oracle:thin:@//<oracle_server>:1521/orcl"
      dc_oracle_icn_jdbc_url: "<Required>"
      ######################################################################################
      ## If the database type is "Db2HADR", then complete the rest of the parameters below.
      ## Otherwise, remove or comment out the rest of the parameters below.
      ######################################################################################
      dc_hadr_standby_servername: "<Required>"
      ## Provide the standby database server port.  For Db2, the default is "50000".
      dc_hadr_standby_port: "<Required>"
      ## Provide the validation timeout.  If not preference, keep the default value.
      dc_hadr_validation_timeout: 15
      ## Provide the retry internal.  If not preference, keep the default value.
      dc_hadr_retry_interval_for_client_reroute: 15
      ## Provide the max # of retries.  If not preference, keep the default value.
      dc_hadr_max_retries_for_client_reroute: 3

    ## The database configuration for UMS (User Management Service)
    dc_ums_datasource:
      ## Provide the datasource configuration for oauth
      ## Possible dc_ums_oauth_type values are "derby" for test only and "db2", "oracle", "sqlserver" and "postgresql" for production.
      ## This configuration should be the same as the other datasource configuration in the dc_ums_datasource section.
      ## db2 with HADR is automatically activated if dc_ums_oauth_alternate_hosts and dc_ums_oauth_alternate_ports are set.
      ## For Oracle RAC, specify the host name of the SCAN listener as the value of the dc_ums_oauth_host parameter
      dc_ums_oauth_type: "<Required>"
      ## Provide the database server name or IP address of the database server.
      dc_ums_oauth_host: "<Required>"
      ## Provide the database server port.  For Db2, the default is "50000".  For Oracle, the default is "1521". For MSSQL, the default is "1433"- For PostgreSQL, the default is "5432".
      dc_ums_oauth_port: "<Required>"
      ## Provide the name of the database for UMS.  For example: "UMSDB"
      dc_ums_oauth_name: "<Required>"
      dc_ums_oauth_schema: OAuthDBSchema
      dc_ums_oauth_ssl: true
      dc_ums_oauth_ssl_secret_name: "<Required>"
      ## For "oracle", "sqlserver" and "postgresql" provide the names of the driver files
      dc_ums_oauth_driverfiles:
      ## For db2 HADR only
      dc_ums_oauth_alternate_hosts:
      dc_ums_oauth_alternate_ports:

      ## Provide the datasource configuration for the teamserver
      ## Possible dc_ums_teamserver_type values are "derby" for test only and "db2", "oracle", "sqlserver" and "postgresql" for production.
      ## This configuration should be the same as the other datasource configuration in the dc_ums_datasource section.
      ## db2 with HADR is automatically activated if dc_ums_teamserver_alternate_hosts and dc_ums_teamserver_alternate_ports are set.
      ## For Oracle RAC, specify the host name of the SCAN listener as the value of the dc_ums_teamserver_host parameter
      dc_ums_teamserver_type: "<Required>"
      dc_ums_teamserver_host: "<Required>"
      ## Provide the database server port.  For Db2, the default is "50000".  For Oracle, the default is "1521". For MS SQL, the default is "1433"- For PostgreSQL, the default is "5432".
      dc_ums_teamserver_port: "<Required>"
      ## Provide the name of the database for UMS teamserver.  For example: "UMSDB"
      dc_ums_teamserver_name: "<Required>"
      dc_ums_teamserver_ssl: true
      dc_ums_teamserver_ssl_secret_name: "<Required>"
      ## For "oracle", "sqlserver" and "postgresql" provide the names of the driver files
      dc_ums_teamserver_driverfiles:
      ## For db2 HADR only
      dc_ums_teamserver_alternate_hosts:
      dc_ums_teamserver_alternate_ports:


  ########################################################################
  ########   IBM Business Automation Navigator configuration      ########
  ########################################################################
  navigator_configuration:

    ## Navigator secret that contains user credentials for LDAP and database
    ban_secret_name: ibm-ban-secret

    ## The architecture of the cluster.  This is the default for Linux and should not be changed.
    arch:
      amd64: "3 - Most preferred"

    ## The number of replicas or pods to be deployed.  The default is 2 replica and for high availability in a production env,
    ## it is recommended to have 2 or more.
    replica_count: 2

    ## This is the image repository and tag that correspond to image registry, which is where the image will be pulled.
    image:

      ## The default repository is the IBM Entitled Registry
      repository: cp.icr.io/cp/cp4a/ban/navigator-sso
      tag: ga-309-icn-if002

      ## This will override the image pull policy in the shared_configuration.
      pull_policy: IfNotPresent

    ## Logging for workloads.  This is the default setting.
    log:
      format: json

    ## This is the initial default resource requests.  If more resources are needed,
    ## make the changes here to meet your requirement.
    resources:
      requests:
        cpu: 500m
        memory: 512Mi
      limits:
        cpu: 1
        memory: 1536Mi

    ## By default "Autoscaling" is enabled with the following settings with a minimum of 1 replca and a maximum of 3 replicas.  Change
    ## this settings to meet your requirement.
    auto_scaling:
      enabled: false
      max_replicas: 3
      min_replicas: 2
      ## This is the default cpu percentage before autoscaling occurs.
      target_cpu_utilization_percentage: 80

    ## Below are the default ICN Production settings.  Make the necessary changes as you see fit.
    icn_production_setting:
      timezone: Etc/UTC
      jvm_initial_heap_percentage: 40
      jvm_max_heap_percentage: 66
      jvm_customize_options:
      icn_jndids_name: ECMClientDS
      icn_schema: ICNDB
      icn_table_space: ICNDB
      allow_remote_plugins_via_http: false


    ## Default settings for monitoring
    monitor_enabled: false
    ## Default settings for logging
    logging_enabled: false

    ## Persistent Volume Claims for Navigator.  The Operator will create the PVC using the names below by default.
    datavolume:
      existing_pvc_for_icn_cfgstore: "icn-cfgstore"
      existing_pvc_for_icn_logstore: "icn-logstore"
      existing_pvc_for_icn_pluginstore: "icn-pluginstore"
      existing_pvc_for_icnvw_cachestore: "icn-vw-cachestore"
      existing_pvc_for_icnvw_logstore: "icn-vw-logstore"
      existing_pvc_for_icn_aspera: "icn-asperastore"

    ## Default values for both rediness and liveness probes.  Modify these values to meet your requirements.
    probe:
      readiness:

        initial_delay_seconds: 120
        period_seconds: 5
        timeout_seconds: 10
        failure_threshold: 6
      liveness:
        initial_delay_seconds: 600
        period_seconds: 5
        timeout_seconds: 5
        failure_threshold: 6

    ## Only use this parameter if you want to override the image_pull_secrets setting in the shared_configuration above.
    image_pull_secrets:
      name: "admin.registrykey"

  ########################################################################
  ########   IBM User and Group Management Service configuration  ########
  ########################################################################
  ums_configuration:
    existing_claim_name:
    dedicated_pods: true
    service_type: Route
    routes_ingress_annotations:
    # your external UMS host name, only required if there is no sc_deployment_hostname_suffix given
    hostname:
    port: 443
    images:
      ums:
        repository: cp.icr.io/cp/cp4a/ums/ums
        tag: 21.0.1
    admin_secret_name: ibm-dba-ums-secret
    ## optional for secure communication with UMS
    external_tls_secret_name:
    ## optional for secure communication with UMS
    external_tls_ca_secret_name:
    ## optional for secure communication with UMS
    external_tls_teams_secret_name:
    ## optional for secure communication with UMS
    external_tls_scim_secret_name:
    ## optional for secure communication with UMS
    external_tls_sso_secret_name:

    use_custom_jdbc_drivers: false
    use_custom_binaries: false
    custom_secret_name:

    oauth:
      ## optional: full DN of an LDAP group that is authorized to manage OIDC clients, in addition to primary admin from admin secret
      client_manager_group:
      ## optional: full DN of an LDAP group that is authorized to manage app_tokens, in addition to primary admin from admin secret
      token_manager_group:
      ## optional: lifetime of OAuth access_tokens. default is 7200s
      access_token_lifetime:
      ## optional: lifetime of app-tokens. default is 366d
      app_token_lifetime:
      ## optional: lifetime of app-passwords. default is 366d
      app_password_lifetime:
      ## optional: maximum number of app-tokens or app-passwords per client. default is 100
      app_token_or_password_limit:
      ## optional: encoding / encryption when storing client secrets in OAuth database. Default is xor for compatibility. Recommended value is PBKDF2WithHmacSHA512
      client_secret_encoding:

    #### If dedicated_pods is set to false, the UMS capabilities sso, scim and teamserver
    #### run in the same pods and share this configuration
    replica_count: 2
    resources:
      limits:
        cpu: 500m
        memory: 512Mi
      requests:
        cpu: 200m
        memory: 256Mi
    autoscaling:
      enabled: true
      min_replicas: 2
      max_replicas: 5
      target_average_utilization: 98
    custom_xml:
    logs:
      traceSpecification: "*=info"

    #### If dedicated_pods is set to true, the UMS capabilities sso, scim and teamserver
    #### run in dedicated pods and are configured separately

    # Configuration for sso pods
    sso:
      replica_count: 2
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 200m
          memory: 256Mi
      autoscaling:
        enabled: true
        minReplicas: 2
        maxReplicas: 5
        targetAverageUtilization: 98
      custom_xml:
      logs:
        traceSpecification: "*=info"

    # configuration for scim pods
    scim:
      replica_count: 2
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 200m
          memory: 256Mi
      autoscaling:
        enabled: true
        minReplicas: 2
        maxReplicas: 5
        targetAverageUtilization: 98
      custom_xml:
      logs:
        traceSpecification: "*=info"

    # configuration for teamserver pods
    teamserver:
      replica_count: 2
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 200m
          memory: 256Mi
      autoscaling:
        enabled: true
        minReplicas: 2
        maxReplicas: 5
        targetAverageUtilization: 98
      custom_xml:
      logs:
        traceSpecification: "*=info"


  ##################################################################
  ########   Resource Registry configuration                ########
  ##################################################################
  resource_registry_configuration:
    # If you inputed hostname and port here. They will be used always
    # If you are using pattern mode (the shared_configuration.sc_deployment_patterns contains value)
    # Then you don't need to fill the hostname and port. It will use shared_configuration.sc_deployment_hostname_suffix to generate one
    # But if you haven't input suffix. And no hostname port assigned. A error will be reported in operator log during deploy
    # For non pattern mode you must assign a valid hostname and port here
    hostname: "{{ 'rr.' + shared_configuration.sc_deployment_hostname_suffix }}"
    port: 443
    images:
      pull_policy: IfNotPresent
      resource_registry:
        repository: cp.icr.io/cp/cp4a/aae/dba-etcd
        tag: 21.0.1
    admin_secret_name: resource-registry-admin-secret
    replica_size: 3
    probe:
      liveness:
        initial_delay_seconds: 60
        period_seconds: 10
        timeout_seconds: 5
        success_threshold: 1
        failure_threshold: 3
      readiness:
        initial_delay_seconds: 10
        period_seconds: 10
        timeout_seconds: 5
        success_threshold: 1
        failure_threshold: 3
    resource:
      limits:
        cpu: "500m"
        memory: "512Mi"
      requests:
        cpu: "100m"
        memory: "256Mi"
    auto_backup:
      enable: true
      minimal_time_interval: 300
      pvc_name: "{{ meta.name }}-dba-rr-pvc"
      log_pvc_name: 'cp4a-shared-log-pvc'
      dynamic_provision:
        enable: true
        size: 3Gi
        size_for_logstore:
        storage_class: "{{ shared_configuration.storage_configuration.sc_fast_file_storage_classname }}"

  #############################################################################
  ## This section contains the BAStudio component configurations              #
  ##  it's the optional component: app_designer, ads_designer, bas,           #
  ##                               workflow-authoring                         #
  #############################################################################
  bastudio_configuration:
    # If you inputed hostname and port here. They will be used always
    # If you are using pattern mode (the shared_configuration.sc_deployment_patterns contains value)
    # Then you don't need to fill the hostname and port. It will use shared_configuration.sc_deployment_hostname_suffix to generate one
    # But if you haven't input suffix. And no hostname port assigned. A error will be reported in operator log during deploy
    # For non pattern mode you must assign a valid hostname and port here
    hostname: "{{ 'bas.' + shared_configuration.sc_deployment_hostname_suffix }}"
    port: 443
    images:
      pull_policy: IfNotPresent
      bastudio:
        repository: cp.icr.io/cp/cp4a/bas/bastudio
        tag: 21.0.1
    #Adjust this one if you created the secret with name other than the default
    admin_secret_name: "{{ meta.name }}-bas-admin-secret"
    #-----------------------------------------------------------------------
    # bastudio admin Secret template will be
    #-----------------------------------------------------------------------
    # apiVersion: v1
    # stringData:
    #   dbPassword: "<Your database password>"
    #   dbUsername: "<Your database username>"
    # kind: Secret
    # metadata:
    #   name: icp4adeploy-bas-admin-secret
    # type: Opaque
    #----------------------------------------
    # Designate an existing LDAP user for the BAStudio admin user.
    admin_user:  "<Required>"
    replica_size: 2
    database:
      # The database type used. Only DB2, Oracle, PostgreSQL supported
      type: "db2"
      # DB2, PostgreSQL - Provide the database server hostname for BAStudio use
      host: "<Required>"
      # DB2, PostgreSQL - Provide the database name for BAStudio use
      # The database provided should be created by the BAStudio SQL script template.
      name: "<Required>"
      # DB2, PostgreSQL - Provide the database server port for BAStudio use
      port: "<Required>"
      # If you want to enable DB2 database automatic client reroute (ACR) for HADR or PostgreSQL Connection Fail-over, you must configure alternative_host and alternative_port. Otherwise, leave them blank.
      alternative_host:
      alternative_port:
      # Enabled SSL for Database is true by default
      ssl_enabled: true
      # Oracle - If you are using Oracle input the oracle database connection URL here
      oracle_url:
      cm_max_pool_size: '50'
      cm_min_pool_size: '2'
      # Enabled the SSL for database is true by default. Please save the TLS certificate used by database in a secret and put the name here
      certificate_secret_name: <Required>
      # If you are using custom JDBC (for example using Oracle or some special DB2 driver). Please set this one to true
      use_custom_jdbc_drivers: false
      # The PVC name which bind to the PV which have the custom JDBC driver files stored
      custom_jdbc_pvc:
      # The custom JDBC file set
      jdbc_driver_files: 'db2jcc4.jar db2jcc_license_cisuz.jar db2jcc_license_cu.jar'
    autoscaling:
      enabled: false
      minReplicas: 1
      maxReplicas: 3
      targetAverageUtilization: 80
    external_connection_timeout: 60s
    # Custom liberty XML configurations
    custom_xml:
    # The secret name which contain custom liberty configurations
    custom_secret_name:
    # The Business Automation Custom XML configurations
    bastudio_custom_xml:
    # If you don't want to use walkme script. You can set this one to false
    use_walkme: true
    max_cached_objects_during_refactoring: 256
    logs:
      # You can find all possible options for this section on liberty document
      # https://www.ibm.com/support/knowledgecenter/SSEQTP_liberty/com.ibm.websphere.liberty.autogen.base.doc/ae/rwlp_config_logging.html
      consoleFormat: 'json'
      consoleLogLevel: 'INFO'
      consoleSource: 'message,trace,accessLog,ffdc,audit'
      traceFormat: 'ENHANCED'
      traceSpecification: '*=info'
      messageFormat: 'SIMPLE'
    tls:
      tlsTrustList: []
    liveness_probe:
      initialDelaySeconds: 300
      periodSeconds: 30
      timeoutSeconds: 5
      failureThreshold: 3
      successThreshold: 1
    readiness_probe:
      initialDelaySeconds: 240
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 5
      successThreshold: 1
    resources:
      bastudio:
        limits:
          cpu: '4'
          memory: '3Gi'
        requests:
          cpu: '300m'
          memory: '1752Mi'
      init_process:
        limits:
          cpu: '500m'
          memory: '512Mi'
        requests:
          cpu: '100m'
          memory: '128Mi'
    csrf_referrer:
      whitelist: ''
    storage:
      # Main switch for the data persistence of BAStudio
      enabled: "true"
      # PVC name used to store the BAStudio server log
      existing_pvc_for_logstore: "cp4a-shared-log-pvc"
      # Size of the PV to store the BAStudio server log. Used when creating the PVC
      size_for_logstore: "10Gi"
      # PVC name used to store the BAStudio server dump files
      existing_pvc_for_dumpstore: "{{ meta.name }}-bastudio-dump-pvc"
      # Size of the PV to store the BAStudio server dump files. Used when creating the PVC
      size_for_dumpstore: "10Gi"
      # PVC name used to store the BAStudio server index files
      existing_pvc_for_index: "{{ meta.name }}-bastudio-index-pvc"
      # Size of the PV to store the BAStudio server index files. Used when creating the PVC
      size_for_index: "10Gi"
      # storage class name used for the PVC when not availble
      storage_class: "{{ shared_configuration.storage_configuration.sc_fast_file_storage_classname }}"
    jms_server:
      image:
        ## Image name for Java Messaging Service container
        repository: cp.icr.io/cp/cp4a/bas/jms
        ## Image tag for Java Messaging Service container
        tag: 21.0.1
        ## Pull policy for Java Messaging Service container
        pull_policy: IfNotPresent
      tls:
        ## TLS secret name for Java Message Service (JMS)
        tls_secret_name: "{{ meta.name }}-bastudio-jms-tls"
      resources:
        limits:
          ## Memory limit for JMS configuration
          memory: "1Gi"
          ## CPU limit for JMS configuration
          cpu: "1000m"
        requests:
          ## Requested amount of memory for JMS configuration
          memory: "256Mi"
          ## Requested amount of CPU for JMS configuration
          cpu: "200m"
      storage:
        ## Whether to enable persistent storage for JMS
        persistent: true
        ## Size for JMS persistent storage
        size: "1Gi"
        ## Whether to enable dynamic provisioning for JMS persistent storage
        use_dynamic_provisioning: true
        ## Access modes for JMS persistent storage
        access_modes:
        - ReadWriteOnce
        ## Storage class name for JMS persistent storage
        storage_class: "{{ shared_configuration.storage_configuration.sc_fast_file_storage_classname }}"
    #-----------------------------------------------------------------------
    #  App Engine Playback Server (playback_server) can be only one instance.
    #  This is different from App Engine
    #  (where application_engine_configuration is a list and you can deploy multiple instances).
    #  You should use different database, admin_secret, hostname for playback server and the application engine servers
    #-----------------------------------------------------------------------
    playback_server:
      images:
        pull_policy: IfNotPresent
        db_job:
          repository: cp.icr.io/cp/cp4a/aae/solution-server-helmjob-db
          tag: 21.0.1
        solution_server:
          repository: cp.icr.io/cp/cp4a/aae/solution-server
          tag: 21.0.1
      # If you inputed hostname and port here. They will be used always
      # If you are using pattern mode (the shared_configuration.sc_deployment_patterns contains value)
      # Then you don't need to fill the hostname and port. It will use shared_configuration.sc_deployment_hostname_suffix to generate one
      # But if you haven't input suffix. And no hostname port assigned. A error will be reported in operator log during deploy
      # For non pattern mode you must assign a valid hostname and port here
      hostname: "{{ 'ae-pbk.' + shared_configuration.sc_deployment_hostname_suffix }}"
      port: 443
      # Inside the admin secret. There are two must fields
      admin_secret_name: "playback-server-admin-secret"
      #-----------------------------------------------------------------------
      # The playback server admin Secret template will be
      #-----------------------------------------------------------------------
      # apiVersion: v1
      # stringData:
      #   AE_DATABASE_PWD: "<Your database password>"
      #   AE_DATABASE_USER: "<Your database username>"
      #   REDIS_PASSWORD: "<Your Redis server password>"
      # kind: Secret
      # metadata:
      #   name: playback-server-admin-secret
      # type: Opaque
      #-----------------------------------------------------------------------
      # Designate an existing LDAP user for the Playback Application Engine admin user.
      # This user ID should be in the IBM Business Automation Navigator administrator role, as specified as appLoginUsername in the Navigator secret. 
      # This user should also belong to the User Management Service (UMS) Teams admin group or the UMS Teams Administrators team. 
      # If not, follow the instructions in "Completing post-deployment tasks for Business Automation Studio and Application Engine" in the IBM Knowledge Center to add it to the Navigator Administrator role and UMS team server admin group. 
      admin_user: <Required>
      external_tls_secret:
      external_connection_timeout: 90s
      replica_size: 2
      ## optional when the database type is Db2, must required when the database type is Oracle, PostgreSQL.
      use_custom_jdbc_drivers: false
      service_type: Route
      autoscaling:
        enabled: false
        max_replicas: 5
        min_replicas: 2
        target_cpu_utilization_percentage: 80
      server_identifier: ""
      database:
        # AE Database host name or IP when the database type is Db2, PostgreSQL.
        host: <Required>
        # AE Database name when the database type is Db2, PostgreSQL.
        name: <Required>
        # AE database port number when the database type is Db2, PostgreSQL.
        port: <Required>
        ## If you setup Db2 HADR or PostgreSQL Connection Fail-over and want to use it, you need to configure alternative_host and alternative_port, or else, leave is as blank.
        ## If more than one server name is specified, delimit the server names with commas (,). The number of values that is specified for alternative_host must match the number of values that is specified for alternative_port.
        alternative_host:
        alternative_port:
        ## Only Db2, Oracle, PostgreSQL are supported.
        type: db2
        ## Required only when the database type is Oracle, both ssl and non-ssl. The format must be purely Oracle descriptor like (DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=<your database host/IP>)(PORT=<your database port>))(CONNECT_DATA=(SERVICE_NAME=<your Oracle service name>))).
        oracle_url_without_wallet_directory:
        enable_ssl: true
        ## Required only when type is Oracle and enable_ssl is true. The format must be purely oracle descriptor. SSO wallet directory must be specified and fixed to (MY_WALLET_DIRECTORY=/shared/resources/oracle/wallet).
        oracle_url_with_wallet_directory:
        ## Required only when enable_ssl is true, when the database type is Db2, Oracle or PostgreSQL
        db_cert_secret_name: <Required>
        ## Required only when type is oracle and enable_ssl is true.
        oracle_sso_wallet_secret_name:
        ## Optional. If it is empty, the DBASB is default when the database type is Db2 or PostgreSQL; the AE_DATABASE_USER set in the admin_secret_name is default when the database type is Oracle.
        current_schema: DBASB
        initial_pool_size: 1
        max_pool_size: 100
        max_lru_cache_size: 1000
        max_lru_cache_age: 600000
        dbcompatibility_max_retries: 30
        dbcompatibility_retry_interval: 10
        ## The persistent volume claim for custom JDBC Drivers if using the custom JDBC drivers is enabled(use_custom_jdbc_drivers is true).
        custom_jdbc_pvc:
      log_level:
        node: info
        browser: 2
      content_security_policy:
        enable: false
        whitelist:
        frame_ancestor:
      env:
        max_size_lru_cache_rr: 1000
        server_env_type: development
        purge_stale_apps_interval: 86400000
        # Number of preview-only automation application must be more to trigger purge,
        apps_threshold: 100
        # Age of preview-only automation application since publish to be stale in milliseconds
        stale_threshold: 172800000
        # Number of preview-only automation services must be more to trigger purge,
        service_threshold: 100
        # Age of preview-only automation service since publish to be stale in milliseconds
        service_stale_threshold: 172800000
        # Service socket connection timeout in milliseconds
        connection_timeout: 1200000
        uv_thread_pool_size: 40
      max_age:
        auth_cookie: "900000"
        csrf_cookie: "3600000"
        static_asset: "2592000"
        hsts_header: "2592000"
      probe:
        liveness:
          failure_threshold: 5
          initial_delay_seconds: 60
          period_seconds: 10
          success_threshold: 1
          timeout_seconds: 180
        readiness:
          failure_threshold: 5
          initial_delay_seconds: 10
          period_seconds: 10
          success_threshold: 1
          timeout_seconds: 180
      #-----------------------------------------------------------------------
      # If you want better HA experience.
      # - Set the session.use_external_store to true
      # - Fill in your redis server information
      #-----------------------------------------------------------------------
      redis:
        # Your external redis host/ip
        host:
        # Your external redis port
        port: '6379'
        ttl: 1800
        # If your redis enabled TLS connection set this to true
        # You should add redis server CA certificate in tls_trust_list or trusted_certificate_list
        tls_enabled: false
      resource_ae:
        limits:
          cpu: 2000m
          memory: 2Gi
        requests:
          cpu: 300m
          memory: 256Mi
      resource_init:
        limits:
          cpu: 500m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      session:
        check_period: "3600000"
        duration: "1800000"
        max: "10000"
        resave: "false"
        rolling: "true"
        save_uninitialized: "false"
        #-----------------------------------------------------------------------
        # If you want better HA experience.
        # - Set the session.use_external_store to true
        # - Fill in your redis server information
        #-----------------------------------------------------------------------
        use_external_store: "false"
      tls:
        tls_trust_list: []
      # If you want to make the replicate size more than 1 for this cluster. Then you must enable the shared storage
      share_storage:
        enabled: true
        # If you create the PV manually. Then please provide the PVC name bind here
        pvc_name:
        auto_provision:
          enabled: true
          # Required if you enabled the auto provision
          storage_class:
          size: 20Gi
      log_storage:
        enabled: true
        pvc_name: 'cp4a-shared-log-pvc'
        log_file_size: '20M'
        log_rotate_size: 5
        auto_provision:
          enabled: true
          # By default it will reuse the operator shared log pvc. If you assgined other name
          # And enabled the auto provision. We will provision that with fast storage class by default
          # If you want to adjust that please fill in this value.
          storage_class: ""
          size: '5Gi'

  ##############################################################################
  ########      IBM Business Automation Insights (BAI) configuration    ########
  ##############################################################################
  bai_configuration:
    persistence:
      # Set this parameter to false to disable dynamic provisioning as the persistence mode for BAI components.
      useDynamicProvisioning: true
    # Name of a secret that is already deployed and contains custom values for configuration parameters.
    # Default value: none.
    bai_secret: ""
    image_credentials:
      # Specific docker registry for the BAI images.
      # If not set, shared_configuration.sc_image_repository is used.
      registry: cp.icr.io/cp/cp4a
    # Image pull policy for BAI images.
    # If not set, shared_configuration.images.pull_policy is used.
    image_pull_policy: "IfNotPresent"
    # An array of image pull secret names for BAI Docker images.
    # If not set, the BAI pods use the pull secrets from shared_configuration.image_pull_secrets.
    image_pull_secrets:
    - ""
    sc_deployment_license: "production"
    # This section allow to enhance the configuration of Kafka clients.
    # Those parameters are not mandatory.
    kafka:
      # Indicates whether event consumption starts at the "earliest" offset or at the "latest" offset.
      # Setting it to "latest" means that events sent before BAI is running are not processed.
      # If you want to process events sent before BAI is running set this parameter to "earliest".
      auto_offset_reset: "latest"
      # You can provide the name of a ConfigMap that is already deployed to Kubernetes
      # and contains Kafka Consumer and producer properties.
      properties_config_map: ""
      # The number of seconds before the socket communication with the Kafka server times out.
      socket_timeout_ms: 10000
    settings:
      # Set it to true to enable Apache Kafka data egress.
      egress: true
      # Set it to true to enable writing events to HDFS.
      hdfs: true
      # Provide configuration of Apache Kafka topics.
      # All topics must be prefixed with icp4ba-bai
      # If not set, topics with default names as below are created.
      ingress_topic: "icp4ba-bai-ingress"
      egress_topic: "icp4ba-bai-egress"
      service_topic: "icp4ba-bai-service"

    # Setup of Elasticsearch for BAI.
    setup:
      image:
        repository: cp.icr.io/cp/cp4a/bai/bai-setup
        tag: 21.0.1
      # The back-off limit property specifies the number of retries before the setup job is considered failed.
      backoff_limit: 6
      resources:
        requests:
          # The minimum memory required, including JVM heap and file system cache, to start the setup pod.
          memory: "50Mi"
          # The minimum amount of CPU required to start the setup pod.
          cpu: "200m"
        limits:
          # The maximum memory, including JVM heap and file system cache, to allocate to the setup pod.
          memory: "120Mi"
      # The runAsUser value for the security context of the setup pod.
      # This is usually a numeric value that corresponds to a user ID.
      # If not set, the value of shared_configuration.sc_run_as_user is used as a default value.
      run_as_user: "{{ shared_configuration.sc_run_as_user }}"

    # The BAI Administration service. Provides business data anonymization services for HDFS storage.
    admin:
      image:
        repository: cp.icr.io/cp/cp4a/bai/bai-admin
        tag: 21.0.1
      # Supported values are ClusterIP and NodePort. Default: ClusterIP.
      service_type: NodePort
      # Set this parameter only if serviceType property is set to NodePort.
      external_port:
      # You can use this parameter to customize the hostname of the administration service route.
      # If not set, the value of shared_configuration.sc_deployment_hostname_suffix is used.
      hostname: "admin.bai.{{ shared_configuration.sc_deployment_hostname_suffix }}"
      # The number of Admin service replicas. For High Availability,
      # use at least 2 replicas.
      replicas: 2
      # The username to the administration service API.
      username: admin
      # The password to the administration service API. It is recommended to use a randomly-generated password.
      password: passw0rd
      # The runAsUser value for the security context of the admin pod.
      # This is usually a numeric value that corresponds to a user ID.
      # If not set, the value of shared_configuration.sc_run_as_user is used as a default value.
      run_as_user: "{{ shared_configuration.sc_run_as_user }}"
      # Optional: Enables SSL with an existing certificate for automatic creation of OpenShift routes.
      # If you don't want to provide an external TLS certificate, leave this empty, the operator will generate one for you.
      external_tls_secret_name: "{{ meta.name }}-bai-admin-external-tls-secret"
      # Optional. The Certificate Authority (CA) used to sign the external TLS secret for automatic creation of OpenShift routes.
      # If you do not want to provide a CA to sign the external TLS certificate, leave this parameter empty.
      external_tls_ca_secret_name:

    management:
      image:
        repository: cp.icr.io/cp/cp4a/bai/bai-management
        tag: 21.0.1
      # You can use this parameter to customize the hostname of the management service route.
      # If not set, the value of shared_configuration.sc_deployment_hostname_suffix is used.
      hostname: "management.bai.{{ shared_configuration.sc_deployment_hostname_suffix }}"
      # The number of Management service replicas. For High Availability,
      # use at least 2 replicas.
      replicas: 2
      # Optional: Enables SSL with an existing certificate for automatic creation of OpenShift routes.
      # If you don't want to provide an external TLS certificate, leave this empty, the operator will generate one for you.
      external_tls_secret_name: "{{ meta.name }}-bai-management-external-tls-secret"
      # Optional. The Certificate Authority (CA) used to sign the external TLS secret for automatic creation of OpenShift routes.
      # If you do not want to provide a CA to sign the external TLS certificate, leave this parameter empty.
      external_tls_ca_secret_name: ""

    flink_pv:
      # The capacity of the persistent volume. Default: "20Gi"
      capacity: "20Gi"
      # If not set, shared_configuration.sc_dynamic_storage_classname is used as a default value.
      storage_class_name: "{{ shared_configuration.storage_configuration.sc_medium_file_storage_classname }}"
      # Provide the name of an existing claim if one is available. By default, a new persitent volume claim is created.
      existing_claim_name: ""

    flink:
      # The logging level for the Flink event processor. Default: info.
      log_level: 'info'
      # The total memory allocation for Apache Flink task managers (in megabytes).
      task_manager_memory: 1536
      # The interval between checkpoints of an Apache Flink jobs (in milliseconds).
      job_checkpointing_interval: 5000
      # The name of a ConfigMap object that is already deployed to Kubernetes and contains RocksDB properties for Flink.
      # Optional. Default: none.
      rocks_db_properties_config_map: ""
      # The batch size for bucketing sink storage (in bytes).
      batch_size: 268435456
      # How frequently the job checks for inactive buckets (in milliseconds)
      check_interval: 300000
      # The minimum amount of time after which a bucket that does not receive
      # new data is considered inactive (in milliseconds).
      bucket_threshold: 900000
      # Indicates whether the Flink storage directory needs to be initialized.
      # The HDFS URL for long-term storage.
      # Example:
      # storage_bucket_url: "hdfs://<node_name>:/bucket_path"
      storage_bucket_url: ""
      # Indicates the user for Flink processors to access HDFS. Default: bai.
      # However, when Kerberos is enabled with HDFS, processing jobs access HDFS with the name of the Kerberos principal.
      hadoop_user: bai
      # The default value is true when shared_configuration.sc_deployment_platform parameter is ROKS, false otherwise.
      init_storage_directory: false

    # The Flink job for ingesting events into HDFS (Hadoop Distributed File System).
    # Disabled by default. Can be enabled by setting ingestion.install to true and
    # by completing the prerequisite configuration for HFDS as documented in the KnowledgeCenter.
    ingestion:
      # Set to true to enable the Flink job for sending events to HDFS.
      install: false
      image:
        repository: cp.icr.io/cp/cp4a/bai/bai-ingestion
        tag: 21.0.1
      # The path to the savepoint or checkpoint from which a job will recover.
      # You can use this path to restart the job from a previous state in case of failure.
      # To use the default workflow of the job, leave this option empty.
      recovery_path: ""
      # The number of parallel instances (task slots) to use for running the processing job.
      parallelism: 2

    # The Flink job for processing BPMN events.
    # Enabled automatically if BAI is selected as an optional component of
    # workflow or workflow-workstreams patterns.
    bpmn:
      # Set to true to enable the Flink job for BAW.
      install: false
      image:
        repository: cp.icr.io/cp/cp4a/bai/bai-bpmn
        tag: 21.0.1
      # The path to the savepoint or checkpoint from which a job will recover.
      # You can use this path to restart the job from a previous state in case of failure.
      # To use the default workflow of the job, leave this option empty.
      recovery_path: ""
      # The number of parallel instances (task slots) to use for running the processing job.
      # For High Availability, use at least 2 parallel instances.
      parallelism: 2
      # The delay in milliseconds before clearing the Flink states used for summary transformation.
      # This value cannot be set to 0 nor be greater than 30 minutes.
      # Otherwise, the default value applies instead.
      end_aggregation_delay: 10000
      # Set this parameter to true if you want time series
      # to be written to Elasticsearch indexes.
      force_elasticsearch_timeseries: false

    # The Flink job for processing BAW Advanced events.
    # Disabled by default. Can be enabled by setting bawadv.install to true.
    bawadv:
      # Set to true to enable the Flink job for BAWAdv.
      install: false
      image:
        repository: cp.icr.io/cp/cp4a/bai/bai-bawadv
        tag: 21.0.1
      # The path to the savepoint or checkpoint from which a job will recover.
      # You can use this path to restart the job from a previous state in case of failure.
      # To use the default workflow of the job, leave this option empty.
      recovery_path: ""
      # The number of parallel instances (task slots) to use for running the processing job.
      # For High Availability, use at least 2 parallel instances.
      parallelism: 2

    # The Flink job for processing ICM events.
    # Enabled automatically if BAI is selected as an optional component of
    # workflow or workflow-workstreams patterns.
    icm:
      # Set to true to enable the Flink job for ICM.
      install: false
      image:
        repository: cp.icr.io/cp/cp4a/bai/bai-icm
        tag: 21.0.1
      # The path to the savepoint or checkpoint from which a job will recover.
      # You can use this path to restart the job from a previous state in case of failure.
      # To use the default workflow of the job, leave this option empty.
      recovery_path: ""
      # The number of parallel instances (task slots) to use for running the processing job.
      # For High Availability, use at least 2 parallel instances.
      parallelism: 2

    # The Flink job for processing ODM events.
    # Enabled automatically if BAI is selected as an optional component of
    # decisions pattern.
    odm:
      # Set to true to enable the Flink job for ODM.
      install: false
      image:
        repository: cp.icr.io/cp/cp4a/bai/bai-odm
        tag: 21.0.1
      # The path to the savepoint or checkpoint from which a job will recover.
      # You can use this path to restart the job from a previous state in case of failure.
      # To use the default workflow of the job, leave this option empty.
      recovery_path: ""
      # The number of parallel instances (task slots) to use for running the processing job.
      # For High Availability, use at least 2 parallel instances.
      parallelism: 2

    # The Flink job for processing Content events.
    # Enabled automatically if BAI is selected as an optional component of
    # content pattern.
    content:
      # Set to true to enable the Flink job for Content.
      install: false
      image:
        repository: cp.icr.io/cp/cp4a/bai/bai-content
        tag: 21.0.1
      # The path to the savepoint or checkpoint from which a job will recover.
      # You can use this path to restart the job from a previous state in case of failure.
      # To use the default workflow of the job, leave this option empty.
      recovery_path: ""
      # The number of parallel instances (task slots) to use for running the processing job.
      # For High Availability, use at least 2 parallel instances.
      parallelism: 2

    # The Flink job for processing events from custom sources and events based on Avro schema.
    # Its configuration is automatic for events emitted by ADS and BA ML Workforce Insights.
    # For custom events, its configuration must be filled in the Custom Resource as documented in
    # the Knowledge Center.
    event_forwarder:
      image:
        repository: cp.icr.io/cp/cp4a/bai/bai-event-forwarder
        tag: 21.0.1
      recovery_path: ""
      # The number of parallel instances (task managers) to use for running the processing job.
      # For High Availability, use at least 2 parallel instances.
      parallelism: 2
      # The list of routing configurations of custom events.
      # The value of this parameter must contain at least one configuration,
      # which is composed of a mandatory Kafka topic name along with
      # an Elasticsearch index or an HDFS bucket, or both.
      configurations:
          # The name of a Kafka topic (source). Must be prefixed by icp4ba-bai
        - kafka_topic: ""
          # The name of an Elasticsearch index (target). Must be prefixed by icp4ba-bai
          elasticsearch_index: ""
          # The name of an HDFS bucket (target)
          hdfs_bucket: ""

    # Kerberos auth for HDFS (optional).
    kerberos:
      # Set this parameter to true to enable Kerberos authentication for HDFS.
      enabled_for_hdfs: false
      # Set the default name of the Kerberos realm.
      realm: ""
      # Set the host of the distribution center for Kerberos keys.
      kdc: ""
      # Set the Kerberos principal to authenticate with.
      principal: ""
      # Set the kerberos base64-encoded keytab
      keytab: ""

    # Configuration of initialization containers.
    init_image:
      image:
        repository: cp.icr.io/cp/cp4a/bai/bai-init
        tag: 21.0.1

    # Business data dashboarding.
    business_performance_center:
      image:
        repository: cp.icr.io/cp/cp4a/bai/bai-monitoring-app
        tag: 21.0.1
      # Set to false to disable Business Performance Center. Default: true.
      install: true
      # The name of a secret that is already deployed to Kubernetes,
      # which contains configuration information for the Business Performance Center.
      # If you leave this field empty and an UMS instance is installed by the Cloud Pak,
      # the configuration information is automatically generated and stored in a default secret.
      config_secret_name: ""
      # The port to which the Business Performance Center service API is exposed.
      external_port: 9443
      # The number of Business Performance Center replicas. For High Availability,
      # use at least 2 replicas.
      replicas: 2
      init_keytool:
        image:
          repository: cp.icr.io/cp/cp4a/ums/dba-keytool-initcontainer
          tag: 21.0.1
      init_ums:
        image:
          repository: cp.icr.io/cp/cp4a/aae/dba-umsregistration-initjob
          tag: 21.0.1
      oidc:
        # The internal communication with single-sign-on (SSO) service.
        # If UMS installation can be reach internally, set this parameter to the UMS SSO service name
        # Otherwise, set it to the SSO external route hostname.
        # If you leave this field empty it will use the default value of the UMS instance installed by the Cloud Pak.
        host: ""
        # The external communication with the UMS single-sign-on (SSO) service.
        # Set this parameter to the SSO external route hostname.
        # If you leave this field empty it will use the default value of the UMS instance installed by the Cloud Pak.
        external_host: ""
        # The host used to retrieve the UMS issuer. Set this parameter to the UMS default route.
        # If you leave this field empty it will use the default value of the UMS instance installed by the Cloud Pak.
        issuer_host: ""
        port: 443
      # Represents the external communication to the UMS team server service. Set this parameter to the team server external route hostname.
      # If you leave this field empty it will use the default value of the UMS instance installed by the Cloud Pak.
      teamserver_host: ""
      # The external communication to the UMS SCIM service. Set this parameter to the SCIM external route hostname.
      # If you leave this field empty it will use the default value of the UMS instance installed by the Cloud Pak.
      scim_host: ""
      # The UUID identifier, which is taken from UMS, of the team that you nominate to be the administration team
      # for Business Performance Center.
      admin_team: ""
      # Set to true if you want to grant all users access to all data.
      all_users_access: false
      # You can use the redirectURIs parameter to specify the route to access Business Performance Center.
      # The URL must end with a forward slash (/).
      # It is not necessary to specify this parameter when relying on the route created by default.
      redirect_uris: ""
      # The URL to which users are redirected when they log out of Business Performance Center.
      # This URL can be the same as the redirectURIs URL.
      # In this case, users still see the same Business Performance Center window but needs to log in
      # again before they can resume working with Business Performance Center.
      logout_redirect_uris: ""
      # For the security context of the Business Performance Center pod,
      # this parameter is usually a numeric value that corresponds to a user ID.
      # If not set, the value of shared_configuration.sc_run_as_user is used as a default value.
      run_as_user: "{{ shared_configuration.sc_run_as_user }}"
      # You can use this parameter to customize the hostname of the Business Performance Center route.
      # If not set, the value of shared_configuration.sc_deployment_hostname_suffix is used.
      hostname: "business-performance-center.bai.{{ shared_configuration.sc_deployment_hostname_suffix }}"
      resources:
        limits:
          # The maximum memory, including JVM heap size and file system cache, to allocate to the Business Performance Center pod.
          # Adjust this parameter value for better resource allocation and better performance.
          memory: "2Gi"
          # The maximum amount of CPU to allocate to the Business Performance Center pod.
          # Adjust this parameter value for better resource allocation and better performance.
          cpu: "2000m"
      # Set this parameter to false if you do not want
      # the Business Performance Center plug-in to be automatically installed into Navigator.
      auto_plugin: true
      # Optional: Enables SSL with an existing certificate for automatic creation of OpenShift routes.
      # If you don't want to provide an external TLS certificate, leave this empty, the operator will generate one for you.
      external_tls_secret_name: "{{ meta.name }}-bai-bperf-external-tls-secret"
      # Optional. The Certificate Authority (CA) used to sign the external TLS secret for automatic creation of OpenShift routes.
      # If you do not want to provide a CA to sign the external TLS certificate, leave this parameter empty.
      external_tls_ca_secret_name: ""


  #############################################################################
  ######## IBM Business Automation Application server  configurations  ########
  ##  This section contains the configurations for                           ##
  ##  * App Engine Server                                                    ##
  ##  it's the optional component and will be installed when                 ##
  ##  patterns include: application, workflow, workstreams,                  ##
  ##                    workflow-workstreams or document_processing          ##
  #############################################################################
  application_engine_configuration:
  ## The application_engine_configuration is a list, you can deploy multiple instances of AppEngine, you can assign different configurations for each instance.
  ## For each instance, application_engine_configuration.name, database, admin_secret_name and hostname must be assigned to different values.
  ## You should use different database, admin_secret_name, hostname for playback server and the application engine servers
  - name: workspace
    images:
      pull_policy: IfNotPresent
      solution_server:
        repository: cp.icr.io/cp/cp4a/aae/solution-server
        tag: 21.0.1
      db_job:
        repository: cp.icr.io/cp/cp4a/aae/solution-server-helmjob-db
        tag: 21.0.1
    # If you inputed hostname and port here. They will be used always
    # If you are using pattern mode (the shared_configuration.sc_deployment_patterns contains value)
    # Then you don't need to fill the hostname and port. It will use shared_configuration.sc_deployment_hostname_suffix to generate one
    # But if you haven't input suffix. And no hostname port assigned. A error will be reported in operator log during deploy
    # For non pattern mode you must assign a valid hostname and port here
    hostname: "{{ 'ae-workspace.' + shared_configuration.sc_deployment_hostname_suffix }}"
    port: 443
    # Inside the admin secret. There are two must fields
    admin_secret_name: "{{ meta.name }}-workspace-aae-app-engine-admin-secret"
    #-----------------------------------------------------------------------
    # The app engine admin Secret template will be
    #-----------------------------------------------------------------------
    # apiVersion: v1
    # stringData:
    #   AE_DATABASE_PWD: "<Your database password>"
    #   AE_DATABASE_USER: "<Your database username>"
    #   REDIS_PASSWORD: "<Your Redis server password>"
    # kind: Secret
    # metadata:
    #   name: icp4adeploy-workspace-aae-app-engine-admin-secret
    # type: Opaque
    #-----------------------------------------------------------------------
    # Designate an existing LDAP user for the Application Engine admin user.
    # This user ID should be in the IBM Business Automation Navigator administrator role, as specified as appLoginUsername in the Navigator secret. 
    # This user should also belong to the User Management Service (UMS) Teams admin group or the UMS Teams Administrators team. 
    # If not, follow the instructions in "Completing post-deployment tasks for Business Automation Studio and Application Engine" in the IBM Knowledge Center to add it to the Navigator Administrator role and UMS team server admin group. 
    admin_user: <Required>
    external_tls_secret:
    external_connection_timeout: 90s
    replica_size: 2
    # data_persistence is for Business Automation Application Data Persistence(ae_data_persistence).
    # If you are using pattern mode, the shared_configuration.sc_deployment_patterns contains value and sc_optional_components contains ae_data_persistence, then you do not need input any value to data_persistence.enable, it is enabled by default.
    # If you are using non-pattern mode, you can set data_persistence.enable to true to enable it.
    # Notes: ae_data_persistence is not supported in demo pattern mode and when AE is as playback server
    data_persistence:
        enable:
        ## If ae_data_persistence is enabled. Then you must input one CPE object store name. If you keep the default object store configuration. Then the default name filled should be AEOS.
        object_store_name: "AEOS"
    ## optional when the database type is Db2, must required when the database type is Oracle, PostgreSQL.
    use_custom_jdbc_drivers: false
    service_type: Route
    autoscaling:
      enabled: false
      max_replicas: 5
      min_replicas: 2
      target_cpu_utilization_percentage: 80
    server_identifier: ""
    database:
      # AE Database host name or IP when the database type is Db2, PostgreSQL.
      host: <Required>
      # AE Database name when the database type is Db2, PostgreSQL.
      #Provide the database name for runtime application engine use
      #Please pay attention that if you selected authoring environment also.
      #The database used by playback server and this one should be different
      name: <Required>
      # AE database port number when the database type is Db2, PostgreSQL.
      port: <Required>
      ## If you setup Db2 HADR or PostgreSQL Connection Fail-over and want to use it, you need to configure alternative_host and alternative_port, or else, leave is as blank.
      ## If more than one server name is specified, delimit the server names with commas (,). The number of values that is specified for alternative_host must match the number of values that is specified for alternative_port.
      alternative_host:
      alternative_port:
      ## Only Db2, Oracle, PostgreSQL are supported.
      type: db2
      ## Required only when the database type is Oracle, both ssl and non-ssl. The format must be purely Oracle descriptor like (DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=<your database host/IP>)(PORT=<your database port>))(CONNECT_DATA=(SERVICE_NAME=<your Oracle service name>))).
      oracle_url_without_wallet_directory:
      enable_ssl: true
      ## Required only when type is Oracle and enable_ssl is true. The format must be purely oracle descriptor. SSO wallet directory must be specified and fixed to (MY_WALLET_DIRECTORY=/shared/resources/oracle/wallet).
      oracle_url_with_wallet_directory:
      ## Required only when enable_ssl is true, when the database type is Db2, Oracle or PostgreSQL
      db_cert_secret_name: <Required>
      ## Required only when type is oracle and enable_ssl is true.
      oracle_sso_wallet_secret_name:
      ## Optional. If it is empty, the DBASB is default when the database type is Db2 or PostgreSQL; the AE_DATABASE_USER set in the admin_secret_name is default when the database type is Oracle.
      current_schema: DBASB
      initial_pool_size: 1
      max_pool_size: 100
      max_lru_cache_size: 1000
      max_lru_cache_age: 600000
      dbcompatibility_max_retries: 30
      dbcompatibility_retry_interval: 10
      ## The persistent volume claim for custom JDBC Drivers if using the custom JDBC drivers is enabled(use_custom_jdbc_drivers is true).
      custom_jdbc_pvc:
    log_level:
      node: info
      browser: 2
    content_security_policy:
      enable: false
      whitelist:
      frame_ancestor:
    env:
      max_size_lru_cache_rr: 1000
      server_env_type: development
      purge_stale_apps_interval: 86400000
      # Number of preview-only automation application must be more to trigger purge,
      apps_threshold: 100
      # Age of preview-only automation application since publish to be stale in milliseconds
      stale_threshold: 172800000
      # Number of preview-only automation services must be more to trigger purge,
      service_threshold: 100
      # Age of preview-only automation service since publish to be stale in milliseconds
      service_stale_threshold: 172800000
      # Service socket connection timeout in milliseconds
      connection_timeout: 1200000
      uv_thread_pool_size: 40
    max_age:
      auth_cookie: "900000"
      csrf_cookie: "3600000"
      static_asset: "2592000"
      hsts_header: "2592000"
    probe:
      liveness:
        failure_threshold: 5
        initial_delay_seconds: 60
        period_seconds: 10
        success_threshold: 1
        timeout_seconds: 180
      readiness:
        failure_threshold: 5
        initial_delay_seconds: 10
        period_seconds: 10
        success_threshold: 1
        timeout_seconds: 180
    #-----------------------------------------------------------------------
    # If you want better HA experience.
    # - Set the session.use_external_store to true
    # - Fill in your redis server information
    #-----------------------------------------------------------------------
    redis:
      # Your external redis host/ip
      host:
      # Your external redis port
      port: '6379'
      ttl: 1800
      # If your redis enabled TLS connection set this to true
      # You should add redis server CA certificate in tls_trust_list or trusted_certificate_list
      tls_enabled: false
    resource_ae:
      limits:
        cpu: 2000m
        memory: 2Gi
      requests:
        cpu: 300m
        memory: 256Mi
    resource_init:
      limits:
        cpu: 500m
        memory: 256Mi
      requests:
        cpu: 100m
        memory: 128Mi
    session:
      check_period: "3600000"
      duration: "1800000"
      max: "10000"
      resave: "false"
      rolling: "true"
      save_uninitialized: "false"
      #-----------------------------------------------------------------------
      # If you want better HA experience.
      # - Set the session.use_external_store to true
      # - Fill in your redis server information
      #-----------------------------------------------------------------------
      use_external_store: "false"
    tls:
      tls_trust_list: []
    # If you want to make the replicate size more than 1 for this cluster. Then you must enable the shared storage
    share_storage:
      enabled: true
      # If you create the PV manually. Then please provide the PVC name bind here
      pvc_name:
      auto_provision:
        enabled: true
        # Required if you enabled the auto provision
        storage_class:
        size: 20Gi
    log_storage:
      enabled: true
      pvc_name: 'cp4a-shared-log-pvc'
      log_file_size: '20M'
      log_rotate_size: 5
      auto_provision:
        enabled: true
        # By default it will reuse the operator shared log pvc. If you assgined other name
        # And enabled the auto provision. We will provision that with fast storage class by default
        # If you want to adjust that please fill in this value.
        storage_class: ""
        size: '5Gi'